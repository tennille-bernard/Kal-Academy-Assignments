{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZPc1PilLJYPp9fJI2xR6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tennille-bernard/Kal-Academy-Assignments/blob/main/Assignment_4_2_TB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing packages**"
      ],
      "metadata": {
        "id": "Xrz4GwioeaFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f3tIK7tbwL_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Google Drive**"
      ],
      "metadata": {
        "id": "xGZ1xVpQem1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCG1LQlMevIv",
        "outputId": "f2b8824b-5f1f-4ef5-80c5-d72760b4d6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovx0lHtgfElo",
        "outputId": "43152cd7-f12a-40a5-dd73-aff8a95629ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**"
      ],
      "metadata": {
        "id": "txWTwwZcfXb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'drive/MyDrive/AI_Deep_Stack_Bootcamp_Data/Assignment 4/4_2/training_set',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,  #orginally 32, but since we only have 16 images, adjusted.\n",
        "    class_mode='binary'  #good or bad image\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jtC1qxnfVtb",
        "outputId": "9a4da9dc-471b-4125-e7f6-cf9e0c781e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    'drive/MyDrive/AI_Deep_Stack_Bootcamp_Data/Assignment 4/4_2/test_set',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=20,   #orginally 32, but since we only have 16 images, adjusted.\n",
        "    class_mode='binary'   #good or bad image\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMgmt7Qy85n3",
        "outputId": "97abd46b-0f92-4673-cfbc-405934ac81de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the CNN**: Initializing the CNN with the following layers:  \n",
        "1.  Convolution\n",
        "2.  Pooling\n",
        "3.  Adding a second layer\n",
        "4.  Flattening\n",
        "5.  Full connection\n",
        "6.  Output layer"
      ],
      "metadata": {
        "id": "krvSbqwDgk2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the CNN\n",
        "cnn = tf.keras.models.Sequential()  #Choice made to use the simplest type of model this time."
      ],
      "metadata": {
        "id": "mQ97PFQlgd3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding 1st Convolution layer, 1st Max Pool layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#2nd Conv and Max Pooling Layers\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#Full connection/hidden layer\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K94Tr1AulXAx",
        "outputId": "189526d2-b9f9-4445-8bcb-2064ec65b70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the CNN**  \n",
        "1. Compiling the CNN\n",
        "2. Using the training set to train and the testing set to evaluate"
      ],
      "metadata": {
        "id": "XU0lkuuR8Wib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BYM3pZt78Vj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x=training_set, validation_data=test_set, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvHn5umU8pkK",
        "outputId": "30724971-5d63-421c-b62e-8a98c47bc92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6875 - loss: 0.6740 - val_accuracy: 0.5000 - val_loss: 1.5527\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.5000 - loss: 1.4180 - val_accuracy: 1.0000 - val_loss: 0.5980\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.6250 - loss: 0.6450 - val_accuracy: 0.5000 - val_loss: 0.7822\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.5000 - loss: 0.7742 - val_accuracy: 0.5000 - val_loss: 0.7271\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.5000 - loss: 0.7263 - val_accuracy: 0.5000 - val_loss: 0.6742\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.5000 - loss: 0.6783 - val_accuracy: 0.5000 - val_loss: 0.6428\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.5000 - loss: 0.6516 - val_accuracy: 1.0000 - val_loss: 0.6340\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.9375 - loss: 0.6482 - val_accuracy: 1.0000 - val_loss: 0.6205\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step - accuracy: 0.8125 - loss: 0.6347 - val_accuracy: 1.0000 - val_loss: 0.5969\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 0.6150 - val_accuracy: 1.0000 - val_loss: 0.5641\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 1.0000 - loss: 0.5926 - val_accuracy: 1.0000 - val_loss: 0.5259\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step - accuracy: 1.0000 - loss: 0.5673 - val_accuracy: 1.0000 - val_loss: 0.4866\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 1.0000 - loss: 0.5301 - val_accuracy: 1.0000 - val_loss: 0.4324\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 1.0000 - loss: 0.4771 - val_accuracy: 1.0000 - val_loss: 0.3818\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 1.0000 - loss: 0.4242 - val_accuracy: 1.0000 - val_loss: 0.3231\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 1.0000 - loss: 0.3819 - val_accuracy: 1.0000 - val_loss: 0.2637\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 1.0000 - loss: 0.3104 - val_accuracy: 1.0000 - val_loss: 0.2092\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 0.2623 - val_accuracy: 1.0000 - val_loss: 0.1565\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 1.0000 - loss: 0.1977 - val_accuracy: 1.0000 - val_loss: 0.1149\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604ms/step - accuracy: 1.0000 - loss: 0.1483 - val_accuracy: 1.0000 - val_loss: 0.0700\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 1.0000 - loss: 0.1086 - val_accuracy: 1.0000 - val_loss: 0.0388\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616ms/step - accuracy: 1.0000 - loss: 0.0671 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 0.0351 - val_accuracy: 1.0000 - val_loss: 0.0136\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 1.0000 - loss: 0.0303 - val_accuracy: 1.0000 - val_loss: 0.0090\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 1.0000 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 0.0068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fcf2e270dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting whether an alert is necessary based on an image**"
      ],
      "metadata": {
        "id": "gBAVauj69i9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "#Classifying img1.jpg\n",
        "test_image1 = image.load_img('drive/MyDrive/AI_Deep_Stack_Bootcamp_Data/Assignment 4/4_2/classify/img1.jpg', target_size=(64, 64))\n",
        "test_image1 = image.img_to_array(test_image1)\n",
        "test_image1 = np.expand_dims(test_image1, axis=0)\n",
        "result = cnn.predict(test_image1)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'alert is necessary'\n",
        "else:\n",
        "  prediction = 'alert is not necessary'\n",
        "\n",
        " print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iGWV7qv8xLm",
        "outputId": "b2f4e58e-efa2-4a59-ae01-061c1fc0842a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "alert is not necessary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifying img2.jpg\n",
        "test_image2 = image.load_img('drive/MyDrive/AI_Deep_Stack_Bootcamp_Data/Assignment 4/4_2/classify/img2.jpg', target_size=(64, 64))\n",
        "test_image2 = image.img_to_array(test_image2)\n",
        "test_image2 = np.expand_dims(test_image2, axis=0)\n",
        "result = cnn.predict(test_image2)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'alert is necessary'\n",
        "else:\n",
        "  prediction = 'alert is not necessary'\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhIWm5By-Rro",
        "outputId": "22489593-6019-4064-f846-bdb5d85c6de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "alert is necessary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYM6darfATxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}