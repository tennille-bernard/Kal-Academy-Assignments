{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4AoZF+b4e/8lc600u8ACm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tennille-bernard/Kal-Academy-Assignments/blob/main/CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import packages**  \n",
        "- in *datasets* from tensorflow, they have access to the cifar10 data (https://www.cs.toronto.edu/~kriz/cifar.html) as well as a few other datasets.  It has training and test images already set up in the dataset."
      ],
      "metadata": {
        "id": "RT6lAEi8t-mz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48lbiJ3OszTh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & preprocess data: Set test images and training images**"
      ],
      "metadata": {
        "id": "0IvTMvweuG4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0   #normalizing the data. This will result in (slightly) blurred images."
      ],
      "metadata": {
        "id": "mUJo8u9huP9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**, which adds variety to the images so that it can recognize more variations in the training images so that it can have a more accurate prediction."
      ],
      "metadata": {
        "id": "kLosHSiCwnQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "datagen.fit(train_images)"
      ],
      "metadata": {
        "id": "YDUpev8Pwu4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old code below"
      ],
      "metadata": {
        "id": "wsBACCd8xPZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "#plt.figure(figsize=(10,10))\n",
        "#for i in range(25):\n",
        "#    plt.subplot(5,5,i+1)\n",
        "#    plt.xticks([])\n",
        "#    plt.yticks([])\n",
        "#    plt.grid(False)\n",
        "#    plt.imshow(train_images[i])\n",
        "#    plt.xlabel(class_names[train_labels[i][0]])\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "DSxBaf62xIJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improved CNN Model with Regularization**  \n",
        "Old code commented out, updated code below"
      ],
      "metadata": {
        "id": "8NiOhFYwtEh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = models.Sequential()\n",
        "#model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "##added regularizer\n",
        "#model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "##changed to 128\n",
        "#model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "##Added another maxpool\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Flatten())  #the number of filter in the flatten layer need to match the number of filters in the last convolution layer.\n",
        "##update\n",
        "#model.add(layers.Dense(128, activation='relu'))\n",
        "##added\n",
        "#model.add(layers.Dropout(0.3))\n",
        "#model.add(layers.Dense(10))"
      ],
      "metadata": {
        "id": "WW5BsPCKsZDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()"
      ],
      "metadata": {
        "id": "_VMCjZ0hsfVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the rewrite below, we created an array with all the steps as layers.  \n",
        "\n",
        "We also added a **regularization** and increased the second **Conv2D Layer** to 128 (this indentifies more features), and added 1 more MaxPooling2D layer after.  \n",
        "\n",
        "**Regularization**: There are 3 types:\n",
        "-\n",
        "\n"
      ],
      "metadata": {
        "id": "E-ZNUpxim16V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "AB1VQjcIuMJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling the model**  \n",
        "- from_logits = True --> the data here is raw values, not log values, so the setting must be true."
      ],
      "metadata": {
        "id": "Kc4RdLUDtNbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "#SparseCategoriacalCrossentropy = used to calculate loss function, but this is best when using multi-class classifications for large images. This also saves your memory."
      ],
      "metadata": {
        "id": "wENczCdgsi9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model with Augmented Data**  \n",
        "by fitting the model (passing the datagen.flow with the training inputs) and creating a new variable, history."
      ],
      "metadata": {
        "id": "Kr3sQt8MtSum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#updated\n",
        "history = model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                    epochs=15, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "LM4urEaLsmOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Training History**  \n",
        "Old code commented out, new code below"
      ],
      "metadata": {
        "id": "n-kCN2sptX8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(history.history['accuracy'], label='accuracy')\n",
        "#plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Accuracy')\n",
        "#plt.ylim([0.5, 1])\n",
        "#plt.legend(loc='lower right')\n",
        "#test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "#print(test_acc)"
      ],
      "metadata": {
        "id": "7C1pSdjEso7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#from tensorflow.keras.preprocessing import image\n",
        "#test_image = image.load_img('deer.jpg', target_size=(32, 32))\n",
        "##updated\n",
        "#test_image = image.img_to_array(test_image) / 255.0\n",
        "#test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "\n",
        "#result = model.predict(test_image)\n",
        "##updated\n",
        "#probabilities = tf.nn.softmax(result)\n",
        "#predicted_class = np.argmax(probabilities)\n",
        "#print(f\"Predicted class: {class_names[predicted_class]} with confidence {probabilities.numpy()[0][predicted_class]:.2f}\")"
      ],
      "metadata": {
        "id": "998TxK-Ysr9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ezGzwEgDudXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss should decreate, accuracy should increase."
      ],
      "metadata": {
        "id": "9_h23Rd1zgA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improved Inference Code**\n"
      ],
      "metadata": {
        "id": "jFK53SDQtjOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image_path):\n",
        "    from tensorflow.keras.preprocessing import image\n",
        "    img = image.load_img(image_path, target_size=(32, 32))\n",
        "    img = image.img_to_array(img) / 255.0 #don't forget to normalize the data\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    logits = model.predict(img)\n",
        "    probabilities = tf.nn.softmax(logits)  #converts large numbers to 1s and 0s\n",
        "    predicted_class = np.argmax(probabilities)\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    print(f\"Predicted class: {class_names[predicted_class]} with confidence {probabilities.numpy()[0][predicted_class]:.2f}\")"
      ],
      "metadata": {
        "id": "CSgXGmuStrHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**"
      ],
      "metadata": {
        "id": "WRjca-vqtnXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_image('deer.jpg')"
      ],
      "metadata": {
        "id": "4p9XhfOdttyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}